# data_loader.py
import yfinance as yf
import pandas as pd
import numpy as np
import requests
import urllib3
import io
import os
from statsmodels.tsa.stattools import adfuller
from config import START_DATE

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def stationarity_audit(df, columns, title):
    print(f"\n[AUDIT] --- {title} ---")
    print(f"{'Variable':<15} | {'ADF Stat':<15} | {'p-value':<10} | {'Status'}")
    print("-" * 65)
    for col in columns:
        series = df[col].dropna()
        if len(series) > 100:
            result = adfuller(series)
            status = " Stationary" if result[1] < 0.05 else " NON-Stationary"
            print(f"{col:<15} | {result[0]:<15.4f} | {result[1]:<10.4f} | {status}")
        else:
            print(f"{col:<15} | INSUFFICIENT DATA")

def fetch_cpu_index(cache_file="cpu_cache.csv"):
    # 1. Check if we already have a clean cache
    if os.path.exists(cache_file):
        print(f"[DATA] Loading Global CPU from local cache...")
        return pd.read_csv(cache_file, index_col=0, parse_dates=True)

    # 2. Try the live fetch with a very short timeout
    print("[DATA] Attempting live fetch (5s timeout)...")
    url = "https://policyuncertainty.com/media/CPU%20index.csv"
    try:
        r = requests.get(url, timeout=5, verify=False)
        if r.status_code == 200:
            cpu_df = pd.read_csv(io.StringIO(r.text))
            cpu_df = cpu_df.iloc[:, :2]
            cpu_df.columns = ['Date', 'Global_CPU']
            cpu_df['Global_CPU'] = pd.to_numeric(cpu_df['Global_CPU'], errors='coerce')
            cpu_df['Date'] = pd.to_datetime(cpu_df['Date'], format='%b-%y', errors='coerce')
            cpu_df.dropna(inplace=True).set_index('Date', inplace=True)
            cpu_daily = cpu_df.resample('D').ffill()
            cpu_daily.to_csv(cache_file)
            return cpu_daily
    except Exception as e:
        print(f"[WARNING] Connection failed: {e}")

    # 3. THE FAILSAFE: Generate a dummy series if all else fails
    print("[DATA] CRITICAL: Using neutral dummy data for Global_CPU to prevent hang.")
    dates = pd.date_range(start=START_DATE, end=pd.Timestamp.now())
    dummy_cpu = pd.DataFrame({'Global_CPU': 100.0}, index=dates)
    dummy_cpu.to_csv(cache_file)
    return dummy_cpu


def fetch_and_clean_data():
    print(f"\n[PIPELINE] Initializing Data Ingestion from {START_DATE}...")

    # 1. Market Data
    assets = {'^NSEI': 'Nifty50', '^VIX': 'VIX', 'CL=F': 'Crude_Oil', '^TNX': 'US_10Y', 'DX-Y.NYB': 'DXY'}
    print(f"[DATA] Downloading Yahoo Finance tickers: {list(assets.values())}")
    raw_data = yf.download(list(assets.keys()), start=START_DATE, progress=False)['Close']

    if isinstance(raw_data.columns, pd.MultiIndex):
        raw_data.columns = raw_data.columns.get_level_values(0)
    raw_data.rename(columns=assets, inplace=True)

    # 2. CPU Data
    cpu_df = fetch_cpu_index()
    raw_data = raw_data.join(cpu_df, how='left')

    print("[DATA] Forward filling missing dates (holidays/weekends)...")
    raw_data.ffill(inplace=True)
    raw_data.dropna(inplace=True)

    # 3. Pre-Transform Audit
    stationarity_audit(raw_data, raw_data.columns, title="PRE-TRANSFORMATION (Raw Prices/Levels)")

    # 4. Strict Asset-Specific Transformations
    print("\n[TRANSFORM] Executing Asset-Specific Transformations...")
    df = pd.DataFrame(index=raw_data.index)

    print("  -> Applying Log Returns: Nifty50, Crude_Oil, DXY")
    df['Log_Ret'] = np.log(raw_data['Nifty50'] / raw_data['Nifty50'].shift(1)) * 100
    df['Crude_Oil_Ret'] = np.log(raw_data['Crude_Oil'] / raw_data['Crude_Oil'].shift(1)) * 100
    df['DXY_Ret'] = np.log(raw_data['DXY'] / raw_data['DXY'].shift(1)) * 100

    print("  -> Applying First Differences: US_10Y, VIX")
    df['US_10Y_Diff'] = raw_data['US_10Y'] - raw_data['US_10Y'].shift(1)
    df['VIX_Diff'] = raw_data['VIX'] - raw_data['VIX'].shift(1)

    print("  -> Applying Percentage Change: Global_CPU")
    df['Global_CPU_Ret'] = raw_data['Global_CPU'].pct_change() * 100

    print("[TRANSFORM] Dropping initial NaN row generated by shift()...")
    df.dropna(inplace=True)

    # 5. Sanity Checks
    missing_count = df.isna().sum().sum()
    print(f"\n[CHECK] Total missing values in final dataset: {missing_count}")
    if missing_count > 0:
        print("âš  WARNING: NaNs detected after transformations.")

    # 6. Post-Transform Audit
    stationarity_audit(df, df.columns, title="POST-TRANSFORMATION")

    print(f"\n[PIPELINE]  Data Module Complete. Final Trading Days: {len(df)}")
    return df

if __name__ == "__main__":
    fetch_and_clean_data()
