# tft_model.py
import lightning.pytorch as pl
from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor
from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss
import torch
import warnings

# Suppress PyTorch Lightning verbose warnings for cleaner console output
warnings.filterwarnings("ignore", category=UserWarning)

def train_tft(df, hidden_size, dropout, learning_rate, seed, max_epochs=150, enable_progress_bar=False, pruning_callback=None):
    print(f"\n[TFT] === Initializing Network Engine (Seed: {seed}) ===")

    # 1. Lock the stochastic engine
    pl.seed_everything(seed, workers=True)

    # 2. Define the exact features generated by data_loader.py
    target_col = "Log_Ret"
    unknown_reals = [
        "Log_Ret",
        "GARCH_Resid",
        "VIX_Diff",
        "US_10Y_Diff",
        "DXY_Ret",
        "Crude_Oil_Ret",
        "Global_CPU_Ret"  # Added the missing CPU feature
    ]

    # 3. Validation Slicing Setup
    print("[TFT] Slicing data for out-of-sample validation...")
    training_cutoff = df["time_idx"].max() - 250

    # 4. Building the Training Dataset
    print(f"[TFT] Allocating Training TimeSeriesDataSet (Time Index <= {training_cutoff})...")
    training_dataset = TimeSeriesDataSet(
        df[lambda x: x.time_idx <= training_cutoff],
        time_idx="time_idx",
        target=target_col,
        group_ids=["group"],
        min_encoder_length=60,
        max_encoder_length=60,
        min_prediction_length=1,
        max_prediction_length=1,
        time_varying_known_categoricals=[],
        time_varying_known_reals=["time_idx"],
        time_varying_unknown_reals=unknown_reals,
        add_relative_time_idx=True,
        add_target_scales=True,
        add_encoder_length=True,
        allow_missing_timesteps=True
    )

    # 5. Building the Validation Dataset (The Illusion Fix)
    print("[TFT] Allocating Validation TimeSeriesDataSet...")
    # We pass the full df but use predict=False so it evaluates all 250 days,
    # not just the last day. stop_randomization ensures deterministic evaluation.
    validation_dataset = TimeSeriesDataSet.from_dataset(
        training_dataset, df, predict=False, stop_randomization=True
    )

    print(f"[TFT] Memory allocated. Train samples: {len(training_dataset)} | Val samples: {len(validation_dataset)}")

    # 6. DataLoaders
    # num_workers=0 is safer for avoiding multi-processing deadlocks in Colab during HPO
    train_dataloader = training_dataset.to_dataloader(train=True, batch_size=32, num_workers=0)
    val_dataloader = validation_dataset.to_dataloader(train=False, batch_size=32, num_workers=0)

    # 7. Model Architecture Compilation
    print(f"[TFT] Compiling Temporal Fusion Transformer (Hidden: {hidden_size}, Dropout: {dropout})...")
    tft = TemporalFusionTransformer.from_dataset(
        training_dataset,
        learning_rate=learning_rate,
        hidden_size=hidden_size,
        attention_head_size=4,
        dropout=dropout,
        hidden_continuous_size=hidden_size // 2,
        output_size=3, # 1%, 50%, 99% quantiles
        loss=QuantileLoss(quantiles=[0.01, 0.5, 0.99]),
        optimizer="adam"
    )

    # 8. Training Execution
    print("[TFT] Engaging PyTorch Lightning Trainer...")
    early_stop_callback = EarlyStopping(
        monitor="val_loss", min_delta=1e-4, patience=10, verbose=False, mode="min"
    )
    callbacks_list = [early_stop_callback]
    if pruning_callback is not None:
        callbacks_list.append(pruning_callback)

    trainer = pl.Trainer(
        max_epochs=max_epochs,
        accelerator="gpu" if torch.cuda.is_available() else "cpu",
        gradient_clip_val=0.1,
        callbacks=callbacks_list,
        deterministic=False,  #used to be true
        enable_progress_bar=enable_progress_bar,
        logger=False
    )

    trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)

    # 9. Metric Extraction
    best_val_loss = early_stop_callback.best_score.item()
    optimal_epochs = early_stop_callback.stopped_epoch - early_stop_callback.patience

    # Handle case where training finished without early stopping
    if optimal_epochs < 0:
        optimal_epochs = max_epochs

    print(f"[TFT]  Training Complete. Best Val Loss: {best_val_loss:.4f} (Stopped at Epoch {optimal_epochs})")

    return tft, trainer, best_val_loss
