# data_loader.py
import yfinance as yf
import pandas as pd
import numpy as np
import requests
import urllib3
import io
import os
from statsmodels.tsa.stattools import adfuller
from config import START_DATE

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def stationarity_audit(df, columns, title):
    print(f"\n[AUDIT] --- {title} ---")
    print(f"{'Variable':<15} | {'ADF Stat':<15} | {'p-value':<10} | {'Status'}")
    print("-" * 65)
    for col in columns:
        series = df[col].dropna()
        if len(series) > 100:
            result = adfuller(series)
            status = " Stationary" if result[1] < 0.05 else "❌ NON-Stationary"
            print(f"{col:<15} | {result[0]:<15.4f} | {result[1]:<10.4f} | {status}")
        else:
            print(f"{col:<15} | INSUFFICIENT DATA")

def fetch_cpu_index(cache_file="cpu_cache.csv"):
    print("[DATA] Attempting to fetch Global CPU Index from policyuncertainty.com...")
    url = "https://policyuncertainty.com/media/CPU%20index.csv"
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        s = requests.get(url, headers=headers, verify=False, timeout=10).content
        cpu_df = pd.read_csv(io.StringIO(s.decode('utf-8')))
        cpu_df = cpu_df.iloc[:, :2]
        cpu_df.columns = ['Date', 'Global_CPU']
        cpu_df['Date'] = pd.to_datetime(cpu_df['Date'], format='%b-%y', errors='coerce')
        cpu_df.dropna(inplace=True)
        cpu_df.set_index('Date', inplace=True)

        cpu_daily = cpu_df.resample('D').ffill()
        cpu_daily.to_csv(cache_file)
        print(f"[DATA]  Live CPU fetched successfully. Cached to {cache_file}.")
        return cpu_daily

    except Exception as e:
        print(f"[ERROR] Live CPU Fetch Failed: {e}")
        if os.path.exists(cache_file):
            print(f"[DATA]  Falling back to local cache: {cache_file}")
            return pd.read_csv(cache_file, index_col=0, parse_dates=True)
        else:
            raise FileNotFoundError("CRITICAL: CPU fetch failed and no local cache exists.")

def fetch_and_clean_data():
    print(f"\n[PIPELINE] Initializing Data Ingestion from {START_DATE}...")

    # 1. Market Data
    assets = {'^NSEI': 'Nifty50', '^VIX': 'VIX', 'CL=F': 'Crude_Oil', '^TNX': 'US_10Y', 'DX-Y.NYB': 'DXY'}
    print(f"[DATA] Downloading Yahoo Finance tickers: {list(assets.values())}")
    raw_data = yf.download(list(assets.keys()), start=START_DATE, progress=False)['Close']

    if isinstance(raw_data.columns, pd.MultiIndex):
        raw_data.columns = raw_data.columns.get_level_values(0)
    raw_data.rename(columns=assets, inplace=True)

    # 2. CPU Data
    cpu_df = fetch_cpu_index()
    raw_data = raw_data.join(cpu_df, how='left')

    print("[DATA] Forward filling missing dates (holidays/weekends)...")
    raw_data.ffill(inplace=True)
    raw_data.dropna(inplace=True)

    # 3. Pre-Transform Audit
    stationarity_audit(raw_data, raw_data.columns, title="PRE-TRANSFORMATION (Raw Prices/Levels)")

    # 4. Strict Asset-Specific Transformations
    print("\n[TRANSFORM] Executing Asset-Specific Transformations...")
    df = pd.DataFrame(index=raw_data.index)

    print("  -> Applying Log Returns: Nifty50, Crude_Oil, DXY")
    df['Log_Ret'] = np.log(raw_data['Nifty50'] / raw_data['Nifty50'].shift(1)) * 100
    df['Crude_Oil_Ret'] = np.log(raw_data['Crude_Oil'] / raw_data['Crude_Oil'].shift(1)) * 100
    df['DXY_Ret'] = np.log(raw_data['DXY'] / raw_data['DXY'].shift(1)) * 100

    print("  -> Applying First Differences: US_10Y, VIX")
    df['US_10Y_Diff'] = raw_data['US_10Y'] - raw_data['US_10Y'].shift(1)
    df['VIX_Diff'] = raw_data['VIX'] - raw_data['VIX'].shift(1)

    print("  -> Applying Percentage Change: Global_CPU")
    df['Global_CPU_Ret'] = raw_data['Global_CPU'].pct_change() * 100

    print("[TRANSFORM] Dropping initial NaN row generated by shift()...")
    df.dropna(inplace=True)

    # 5. Sanity Checks
    missing_count = df.isna().sum().sum()
    print(f"\n[CHECK] Total missing values in final dataset: {missing_count}")
    if missing_count > 0:
        print("⚠ WARNING: NaNs detected after transformations.")

    # 6. Post-Transform Audit
    stationarity_audit(df, df.columns, title="POST-TRANSFORMATION")

    print(f"\n[PIPELINE]  Data Module Complete. Final Trading Days: {len(df)}")
    return df

if __name__ == "__main__":
    fetch_and_clean_data()
